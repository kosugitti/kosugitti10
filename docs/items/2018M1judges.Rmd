---
title: "『M-1グランプリ』上沼恵美子さんの採点は本当に偏っていたのかを検証する"
author: ''
date: '2018-12-04'
slug: 2018M1judges
categories:
  - Notes
tags:
  - R
banner: ''
description: ''
images: []
menu: ''
---

## はじめに

この記事は[Stan Advent Calendar2018](https://qiita.com/advent-calendar/2018/stan)，12月04日のエントリー記事です。

[12月01日のアドカレ](https://kosugitti.github.io/kosugitti10/2018/12/01/m1/)で，スーパーマラドーナが優勝するぜ！2本目にいける確率は87%だぜ！とか言っておきながら，大いに外してしまったのですが，この教訓を元に新年をベイズ更新するつもりです（あるいはもっといいモデルを誰か書いてくれ！）

<!--more-->

```{r,echo=FALSE,include=FALSE}
# 分析パッケージを読み込みます。
library(tidyverse)
## Macユーザの呪文です
old = theme_set(theme_gray(base_family="HiraKakuProN-W3"))
# rstanも読み込みます
library(rstan)
rstan_options(auto_write = TRUE)
options(mc.cores = parallel::detectCores())
library(bayesplot)
library(tidybayes)
```

## 感想戦

個人の感想ですが，今年のM-1は，番組が進むにつれてグングン面白さがあがっていく！という感じがなかったなあ，と思っています。かまいたち！ジャルジャル！ときたあとでギャロップがちょっといまいちだったかな，ユニバースが盛り返すかな？！と思ったけどそれほどでもなかったかな，ミキ↑？トムブラウン↓？うーん，という感じ。

でも結果として2本目に残った三組，優勝者，いずれも実力・内容ともに不満があるものではなく，今年も良い芸の祭典だったと思っています。

ところがどうやら，ネットの評価では，審査員がわかってないんじゃないの，というような声も聞こえます。まあそれを言っちゃあ大会が成立しないし，第一回のことを考えれば素人の人気票みたいなのを入れると芸の評価次元ではないところで大きな差がついたりして(おぎやはぎの大阪でのポイントを思い出してください)，それはそれでよくないと思いますが。

## 審査員たちの評価

さて，ネットでは，こんな記事を見かけました。

[『M-1グランプリ』上沼恵美子さんの採点は本当に偏っていたのか](https://www.buzzfeed.com/jp/tatsunoritokushige/m-1)

この記事がいいなあ，と思ったのはえみちゃんを擁護していたところではなくて，分析にあたって標準偏差に注目したところ。データは散らばりが命ですよ。平均値だけではわからんのです。それをちゃんと表現してくれてます。

ただ気になったのは，標準偏差を出して「ね，そんなに違わないよね」で終わっているところ。そんなに，って，どんなになの？どれぐらいなら大きい違いで，どれぐらいなら許容される違いなの？というところが気になって仕方ない。

ということで，「（散らばりの）違いがどれぐらい大きいの」ということをしっかりと検証してみようと思います。

## モデル

審査員のブレについては，前掲の[12月01日のアドカレ](https://kosugitti.github.io/kosugitti10/2018/12/01/m1/)でも扱いました。先の記事は審査員の採点の標準偏差のお話で，審査員$j$の評価は平均$\mu_j$，標準偏差$\sigma_j$の正規分布を仮定しているのかな，と読みましたが，本稿では「お笑い芸人$i$の実力$\theta_i$に伴って採点がなされるが，その採点には誤差$\phi_j$がつき，その誤差は審査員$j$ごとに異なる」というモデルで議論します。このモデルは，12/01のアドカレ記事でできているので，コードは同じになります。

```{r, echo=FALSE, eval=TRUE}
paste(readLines("m1ord.stan"), collapse = "\n") %>% cat()
```

## データ

前回の記事では，M-1本戦前でしたので，敗者復活が誰かもわからなかったし，得点もわからないので欠損値として扱われていました。今回，今年のデータを追加した新しいデータファイルM1scoreOrd2.csvを用意しました。[このデータもOSF](https://osf.io/wym5v/)に置かせてもらっているので，欲しい人は使ってください。

## 推定しちゃうぞ

推定は二段階で行うこととします。まず第一弾として，昨年度までのデータを使って推定を行います。第二弾は，今年度のデータ「だけ」をつかって推定します。これらを比較することで，*今年は特に何が違ったのか*が明らかになると考えられるからです。

データの整形のコードは次の通りです。
```{r}
# 今年の審査員の名前を用意します
judge <- c("オール巨人","上沼恵美子","富澤たけし","立川志らく",
                           "塙宣之","中川礼二","松本人志")
# ファイルを読み込みます
m1 <- read_csv("M1scoreOrd2.csv",na=".")
# データを縦長にします
m1 %>% tidyr::gather(審査員,val,-年代,-演者,-ネタ順,factor_key=TRUE) %>% 
  # 欠損値を削除します
  na.omit %>% 
  # 2018年度のデータを除外します
  dplyr::filter(年代!=18) %>% 
  # factor型にします
  mutate(演者 = factor(演者)) %>% 
  # 審査員の最初の7人を今年の人たちに
  mutate(査員 = fct_relevel(.$審査員,judge )) -> m1Set_1

m1 %>% tidyr::gather(審査員,val,-年代,-演者,-ネタ順,factor_key=TRUE) %>% 
  # 欠損値を削除します
  na.omit %>% 
  # 2018年度のデータだけにします
  dplyr::filter(年代==18) %>% 
  # factor型にします
  mutate(演者 = factor(演者)) %>% 
  # 審査員の最初の7人を今年の人たちに
  mutate(審査員 = fct_relevel(.$審査員,judge )) -> m1Set_2
```

### 推定

以上のデータを次のコードで推定しました。
```{r stan1, echo=TRUE}
# 2018年度まで
dataset1 <- list(L = NROW(m1Set_1),
                P = max(as.numeric(m1Set_1$演者)),
                R = max(as.numeric(m1Set_1$審査員)),
                Pid = as.numeric(m1Set_1$演者),
                Ord = m1Set_1$ネタ順,
                Rid = as.numeric(m1Set_1$審査員),
                Y = m1Set_1$val)
# 2018年度
dataset2 <- list(L = NROW(m1Set_2),
                P = max(as.numeric(m1Set_2$演者)),
                R = max(as.numeric(m1Set_2$審査員)),
                Pid = as.numeric(m1Set_2$演者),
                Ord = m1Set_2$ネタ順,
                Rid = as.numeric(m1Set_2$審査員),
                Y = m1Set_2$val)

model <- stan_model('m1ord.stan')
fit1 <- sampling(model,dataset1,iter=40000,warmu=15000)
fit2 <- sampling(model,dataset2,iter=40000,warmu=15000)
```

##  結果と考察

さて，推定値を見て結果と考察です。まずは昨年度までのデータを使った審査員のブレ，つまり評定の標準偏差はどれぐらいあるものなのかの散らばりを見てみます。
```{r}

fit1 %>% tidybayes::spread_draws(phi_mu) %>% median_hdci() %>% 
  mutate(審査員="これまでの平均") %>% 
  rename(phi=phi_mu) %>% 
  dplyr::select(審査員,phi, .lower, .upper) -> fit1summary
print(fit1summary)
```
これを見ると，平均値(EAP)が4.37で，95%の確信区間が2.70から6.08，これがM1の歴史でいう審査員のブレの大きさだと見積もれます。

つづいて「今年はどうだったか」です。[昨日のアドカレ](https://ytake2.github.io/takeblog/2018/12/03/tidybayes/)で学んだことを生かして，tidybaesパッケージで描いてみました。

```{r}
fit2 %>% tidybayes::spread_draws(phi[r]) %>% 
  ggplot()+geom_halfeyeh(aes(x=phi,y=factor(r,labels=judge)),                                                                 point_interval=median_hdci,.width=c(0.50,0.95)) +
  xlab("審査員のブレの大きさ")+ylab("審査員")
```

この図は黒丸で中央値を，太い線で50%タイルを，細い線で95%タイルを表し，その上に密度をつけているものです。
これを見ると，確かにえみちゃんはブレが大きい方であることがわかります。

数字で見る方がよりはっきりしますね。中央値と95％HDCIを算出し，並べ替えてみました。
```{r}
fit2 %>% tidybayes::spread_draws(phi[r]) %>% median_hdci() %>% 
  mutate(審査員=judge) %>% 
  dplyr::select(審査員, phi, .lower, .upper) %>% 
  dplyr::arrange(phi) -> fit2summary
print(fit2summary)
```
ブレが少ないのがオール巨人，ブレが大きいのが上沼恵美子，ということがわかります。えみちゃんに次いでブレが大きいのは志らく師匠ですねえ。

「ほらみろ，やっぱりブレまくってんじゃねえか，なんだあの審査員は！」という人もいるかもしれませんが，注意してほしいのはこれまでと比べてそれほど特殊な状況ではない，ということです。

審査員がどの程度ブレたか，という推定値(with 95%HDCI)に，これまでのブレの95%区間を点線で書き加えてみたのが次の図になります(青はこれまでのブレのMAP推定値です)。

```{r}
fit2summary%>% as.tibble %>% 
  mutate(審査員 = fct_relevel(.$審査員,judge )) %>% 
  ggplot(aes(x=審査員,y=phi,color=審査員))+geom_point() +xlab("審査員")+ylab("審査員のブレの大きさ")+
  geom_errorbar(aes(ymax=.upper,ymin=.lower,width=0.3))+ylim(0,10)+
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) + 
  geom_hline(yintercept = fit1summary$phi,color="blue",lty=2,alpha=0.5) + 
  geom_hline(yintercept = fit1summary$.lower,color="black",lty=2,alpha=0.5) + 
  geom_hline(yintercept = fit1summary$.upper,color="black",lty=2,alpha=0.5)

```

これをみると，この程度のブレが生じることは「あり得る範囲」の話であって，もしMAP推定値を代表値として考えるなら，むしろオール巨人師匠やサンドウィッチマン富澤がブレなさすぎたぐらい，ということがわかります(それでも彼らの中では95%でこういうことがある範囲です)。

## 結論

「採点は本当に偏っていたのか」というお題を，「採点の偏りの大きさはどの程度か」という意味にして分析したのが今回の結果です。ここでの偏りは，評価軸という意味ではありませんので注意してください。そしてここでの意味で(芸人の実力を中心に散らばる評価の程度という意味で)，今年の採点も例年通り，特に逸脱した評価をしているわけではないということが示されました。

面白ポイントは，標準偏差を考察の対象にしているところです。一般的な統計的検定では，平均値を比較することがほとんどですが，**標準偏差の大きさを比較するようなことができる**のもベイズモデリングの面白いところだ，と思っていただければ。そしてそうした分析が簡単にできてしまうんですね，**そう，Stanならね！**

Enjoy Bayesian Modeling & Merry Christmas, again!

## うんちく

ここからはエビデンスもモデリングも関係ない話，おっさんのうんちくを少し。

審査員が「好みじゃない」とか「笑えなかった」という言葉をつかい，その人たちの点数のつけ方が自分の判断と合わないということがあると，ついあの審査員がおかしい，というように考えてしまいがちですが，私の意見は違います。

第一に，ずっとお笑い，芸能界，演芸の場に立っている人たちが私たちと同じような判断をするはずがないのです。審査員はその芸歴の中で，独自の評価基準を作ってきているのですから，自分と違って当然です。むしろあの人はどういう観点で評価しているのか，と興味を持つべきです。

第二に，そうした独自の観点で発せられるセリフを切り取り，抜き出して，素人の理解の範疇で受け取ってはいけないと思うのです。言葉で表現するときの，その言葉のチョイスも独特のものがあり，例えば好みじゃない，と言ってあげた方が優しい，ということもあるでしょう。例えばかつて談志師匠がテツトモに対して「ここはお前らが来る場所じゃねえ」と言ったのは，私はむしろ優しさだったと思いますねえ。

もしそうした批判をするならば，視聴者自身，自分で採点をするべきです。自分の中で色々な観点があると思いますがそれを0-100の一次元の数値に落とし込むことがいかに難しいか，やってみればすぐにわかります。
そしてその上で，自分の評価とほかの審査員の評価の相関を計算すれば，自分と似た評価軸の人がいることがわかります。

あるいは，今回のデータも使えるのですが，審査員の評定値データから相関行列を計算し，因子分析をすると，お笑いが何次元で評価されているかがわかります。
別解として，評定の類似度＝距離をもとに多次元尺度構成法によって，審査員の評価次元数，審査員のグルーピングを行うこともできます。

参考までに，今回のデータでMDSをされた例をツイッターでみかけましたので，[どうぞ](https://twitter.com/M123Takahashi/status/1069749304351289344)。

